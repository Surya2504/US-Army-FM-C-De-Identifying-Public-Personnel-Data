---
title: "MODELS"
output: html_document
editor_options: 
chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}

library(tidyverse)
library(tidymodels)
library(vip)
library(kknn)

identical_df <- read.csv("/Users/suryamanitejendla/Downloads/DAEN-690-main/identical123.csv",stringsAsFactors=T)

identical_df

```

```{r}
identical_df <- identical_df[ , ! names(identical_df) %in% c("DATECODE", "EMPLOYMENT","PP", "DATECODE1", "EMPLOYMENT1","PP1")]  
identical_df
```

```{r}
#levels
levels(identical_df$Salary_hike)
set.seed(350)

#splitting data into training and test data
df_split <- initial_split(identical_df, prop = 0.75, 
                             strata = Salary_hike)

df_training<- df_split %>% training()

df_test <- df_split %>% testing()

#Feature engineering
df_recipe <- recipe(Salary_hike~ ., data = df_training) %>% 
                step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
                step_normalize(all_numeric(), -all_outcomes()) %>% 
                step_dummy(all_nominal(), -all_outcomes())
df_recipe %>% 
  prep(training = df_training) %>% 
  bake(new_data = NULL)
```


```{r}
#Logistic Regression

logistic_model <- logistic_reg() %>% 
                  set_engine('glm') %>% 
                  set_mode('classification')

#Creating a workflow
df_workflow <- workflow() %>% 
            add_model(logistic_model) %>% 
            add_recipe(df_recipe)
#Fitting the model
df_logistic_fit <- df_workflow %>% 
                      fit(data = df_training)
#Exploring the trained model
df_trained_model <- df_logistic_fit %>% 
                       pull_workflow_fit()
#Variable IMportance plot
vip(df_trained_model)

```


```{r}
#Performance evaluation for predicted categories
predictions_categories <- predict(df_logistic_fit, 
                                  new_data = df_test)

predictions_categories

```


```{r}
#Predicted probabilities
predictions_probabilities <- predict(df_logistic_fit, 
                                     new_data = df_test, 
                                     type = 'prob')

predictions_probabilities

```


```{r}
test_results <- df_test %>% select(Salary_hike) %>% 
                bind_cols(predictions_categories) %>% 
                bind_cols(predictions_probabilities)

test_results

```



```{r}
conf_mat(test_results, 
         truth = Salary_hike, 
         estimate = .pred_class)
```



```{r}

#sensitivity performance metric
sens(test_results,
     truth = Salary_hike, 
     estimate = .pred_class)

```


```{r}
#specificity performance metric
spec(test_results,
     truth = Salary_hike, 
     estimate = .pred_class)
```

```{r}
roc_curve(test_results, 
          truth = Salary_hike,
          estimate = .pred_YES)
```

```{r}
#ROC curve plot
roc_curve(test_results, 
          truth = Salary_hike, 
          estimate = .pred_YES) %>% 
  autoplot()

```


```{r}

# ROC AUC
roc_auc(test_results,
        truth = Salary_hike, 
        .pred_YES)
```

```{r}
#F1 score
f_meas(test_results,
       truth = Salary_hike, 
       estimate = .pred_class)
```

```{r}
#Accuracy of the model

my_metrics <- metric_set(accuracy)

my_metrics(test_results, 
           truth = Salary_hike, 
           estimate = .pred_class,
           .pred_YES)

```

# Model 2

```{r}
##Random forest

rf_model <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
            set_engine('ranger', importance = "impurity") %>% 
            set_mode('classification')
rf_workflow <- workflow() %>% 
               add_model(rf_model) %>% 
               add_recipe(df_recipe)
```

```{r}
## Create a grid of hyper parameter values to test

set.seed(350)

rf_grid <- grid_random(mtry() %>% range_set(c(2, 4)),
                       trees(),
                       min_n(),
                       size = 10)
# View grid
rf_grid
```

```{r}
## Tune random forest workflow
set.seed(350)
df_folds <- vfold_cv(df_training, v = 5)
rf_tuning <- rf_workflow %>% 
             tune_grid(resamples = df_folds,
                       grid = rf_grid)

## Show the top 5 best models based on roc_auc metric
rf_tuning %>% show_best('roc_auc')

```

```{r}
## Select best model based on roc_auc
best_rf <- rf_tuning %>% 
           select_best(metric = 'roc_auc')

# View the best parameters
best_rf
```

```{r}
#finalize workflow
final_rf_workflow <- rf_workflow %>% 
                     finalize_workflow(best_rf)

#variable importance by fitting the data
rf_wf_fit <- final_rf_workflow %>% 
             fit(data = df_training)
rf_fit <- rf_wf_fit %>% 
          pull_workflow_fit()
vip(rf_fit) #plot for variable importance

```

```{r}
#train and evaluate with last fit
rf_last_fit <- final_rf_workflow %>% 
               last_fit(df_split)
#performance metrices on the test data
rf_last_fit %>% collect_metrics()
```

```{r}
#ROC CURVE
rf_last_fit %>% collect_predictions( )%>%
roc_curve(truth=Salary_hike, estimate = .pred_YES) %>% 
autoplot()

```

```{r}
#confusion matrix
rf_predictions <- rf_last_fit %>% collect_predictions()

conf_mat(rf_predictions, truth = Salary_hike, estimate = .pred_class)
```

```{r}
# ROC AUC
roc_auc(rf_predictions, truth = Salary_hike, .pred_YES)
```

```{r}
#accuracy
my_metrics<- metric_set(accuracy)
my_metrics(rf_predictions,truth=Salary_hike,estimate = .pred_class, .pred_YES)

```





